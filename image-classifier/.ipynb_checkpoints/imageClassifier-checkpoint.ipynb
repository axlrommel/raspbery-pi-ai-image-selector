{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /Library/Python/3.7/site-packages (from keras) (5.3.1)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.5.3-cp37-cp37m-macosx_10_9_x86_64.whl (28.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.7 MB 31.5 MB/s eta 0:00:01   |▌                               | 460 kB 8.8 MB/s eta 0:00:04     |█████▌                          | 4.9 MB 8.8 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from keras) (1.19.1)\n",
      "Requirement already satisfied: six in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from h5py->keras) (1.12.0)\n",
      "Installing collected packages: h5py, scipy, keras\n",
      "Successfully installed h5py-2.10.0 keras-2.4.3 scipy-1.5.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (4.4.0.42)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from opencv-python) (1.19.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflowjs\n",
      "  Downloading tensorflowjs-2.6.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 407 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow<3,>=2.1.0\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.1 MB 17 kB/s s eta 0:00:01  |▊                               | 3.8 MB 3.5 MB/s eta 0:00:46     |█████████                       | 46.4 MB 18.9 MB/s eta 0:00:07     |█████████████████               | 87.8 MB 22.4 MB/s eta 0:00:04     |███████████████████             | 98.2 MB 22.4 MB/s eta 0:00:03     |███████████████████████████▎    | 140.9 MB 19.3 MB/s eta 0:00:02     |████████████████████████████▋   | 147.7 MB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<3,>=2.8.0 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from tensorflowjs) (2.10.0)\n",
      "Requirement already satisfied: six<2,>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflowjs) (1.12.0)\n",
      "Collecting tensorflow-hub<0.10,>=0.7.0\n",
      "  Downloading tensorflow_hub-0.9.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-macosx_10_9_x86_64.whl (15.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.1 MB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 6.1 MB/s eta 0:00:01     |██████████▊                     | 2.3 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.1-cp37-cp37m-macosx_10_9_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 14.9 MB/s eta 0:00:01     |███████████████▌                | 1.6 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.13.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 16.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 27.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/site-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.33.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Python/3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.2.2)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.22.1-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools>=41.0.0\n",
      "  Downloading setuptools-50.3.2-py3-none-any.whl (785 kB)\n",
      "\u001b[K     |████████████████████████████████| 785 kB 17.3 MB/s eta 0:00:01   |█▎                              | 30 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 5.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Library/Python/3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.6.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Python/3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/rommelvillagomez/Library/Python/3.7/lib/python/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=7b47d99f1a85a6b1b1f8f9f267750d32ac288a1c4714fafb9324eb6edaad9b92\n",
      "  Stored in directory: /Users/rommelvillagomez/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: numpy, opt-einsum, absl-py, werkzeug, pyasn1-modules, setuptools, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, protobuf, tensorboard-plugin-wit, grpcio, tensorboard, google-pasta, astunparse, keras-preprocessing, gast, tensorflow-estimator, termcolor, tensorflow, tensorflow-hub, tensorflowjs\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.22.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.33.1 keras-preprocessing-1.1.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 setuptools-50.3.2 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 tensorflow-hub-0.9.0 tensorflowjs-2.6.0 termcolor-1.1.0 werkzeug-1.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install opencv-python\n",
    "!pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-607753605151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "file_list = []\n",
    "class_list = []\n",
    "\n",
    "DATADIR = \"data\"\n",
    "\n",
    "# All the categories you want your neural network to detect\n",
    "CATEGORIES = [\"cat\", \"dog\"]\n",
    "\n",
    "# The size of the images that your neural network will use\n",
    "IMG_SIZE = 30\n",
    "\n",
    "# Checking or all images in the data folder\n",
    "for category in CATEGORIES :\n",
    "    path = os.path.join(DATADIR, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES :\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try :\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "X = [] #features\n",
    "y = [] #labels\n",
    "\n",
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "# Creating the files containing all the information about your model\n",
    "pickle_out = open(\"X.pickle\", \"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\", \"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_in = open(\"X.pickle\", \"rb\")\n",
    "X = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Opening the files about data\n",
    "X = pickle.load(open(\"X.pickle\", \"rb\"))\n",
    "y = pickle.load(open(\"y.pickle\", \"rb\"))\n",
    "y = np.array(y) #https://github.com/tensorflow/tensorflow/issues/38613\n",
    "\n",
    "# normalizing data (a pixel goes from 0 to 255)\n",
    "X = X/255.0\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "# 3 convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), input_shape = X.shape[1:]))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# hidden layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "# The output layer with 2 neurons, for 2 classes\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Compiling the model using some basic parameters\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "# validation_split corresponds to the percentage of images used for the validation phase compared to all the images\n",
    "history = model.fit(X, y, batch_size=32, epochs=20, validation_split=0.1, callbacks=[callback])\n",
    "\n",
    "# Saving the model\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file :\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# python\n",
    "model.save_weights(\"model.h5\")\n",
    "model.save('CNN.model')\n",
    "\n",
    "#javascript\n",
    "saved_model_path = \"./model.js.h5\"\n",
    "model.save(saved_model_path)\n",
    "!tensorflowjs_converter --input_format=keras {saved_model_path} ./\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"CNN.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"cat\", \"dog\"]\n",
    "\n",
    "def prepare(file):\n",
    "    IMG_SIZE = 30\n",
    "    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "image = prepare(\"./samples/cat.123.jpg\")\n",
    "prediction = model.predict([image])\n",
    "prediction = list(prediction[0])\n",
    "print(prediction)\n",
    "print(CATEGORIES[prediction.index(max(prediction))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
